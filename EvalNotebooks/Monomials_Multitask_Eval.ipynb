{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
    "from samplers import get_data_sampler\n",
    "from tasks import get_task_sampler, HaarWavelets\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from train import get_all_deg2_term_indices\n",
    "\n",
    "\n",
    "sns.set_theme('notebook', 'darkgrid')\n",
    "palette = sns.color_palette('colorblind')\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "run_dir = \"../models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference solver functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_linear_regression_rerefence_errors(xs, ys):\n",
    "    # Least Squares Optimization\n",
    "    lsq_errors = []\n",
    "    for i in tqdm(range(1, xs.shape[1])): # 1-19\n",
    "        preds = []\n",
    "        for batch_id in range(xs.shape[0]): # 0-63\n",
    "            preds.append(\n",
    "            # fit n_points -1 regressors for each entry in batch\n",
    "            LinearRegression(fit_intercept = False).fit(xs[batch_id,:i], ys[batch_id,:i])\\\n",
    "                .predict(xs[batch_id,i:i+1])[0]\n",
    "            )\n",
    "        preds = np.array(preds).squeeze()\n",
    "        lsq_errors.append(((ys[:,i] - preds)**2).mean(axis = 0).numpy())\n",
    "    return np.array(lsq_errors)\n",
    "\n",
    "\n",
    "def transform_features_to_only_deg2_monomials(data):\n",
    "    # data -> n_points x n_dim_input\n",
    "    n_dim_inp = data.shape[1]\n",
    "    trans = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "    data_interac_only = trans.fit_transform(data) # n_points x no_of_interac_only_terms\n",
    "    data_sq_and_interac = np.concatenate(\n",
    "        ((data_interac_only[:,1:(n_dim_inp+1)])**2, data_interac_only[:,(n_dim_inp+1):]),\n",
    "        axis=1) # n_points x no_of_self_sq_and_interac_only_terms\n",
    "    return data_sq_and_interac\n",
    "\n",
    "\n",
    "def transform_features_to_upto_deg2_monomials(data):\n",
    "    # data -> n_points x n_dim_input\n",
    "    n_dim_inp = data.shape[1]\n",
    "    trans = PolynomialFeatures(degree=2)\n",
    "    data_upto_deg2_feats = trans.fit_transform(data) # n_points x no of deg 0, deg1 and deg 2 terms\n",
    "    # data_interac_only = trans.fit_transform(data) # n_points x no_of_interac_only_terms\n",
    "    # data_sq_and_interac = np.concatenate(\n",
    "    #     ((data_interac_only[:,1:(n_dim_inp+1)])**2, data_interac_only[:,(n_dim_inp+1):]),\n",
    "    #     axis=1) # n_points x no_of_self_sq_and_interac_only_terms\n",
    "    return data_upto_deg2_feats\n",
    "\n",
    "def transform_features_to_only_fixedS_deg2_monomials(data, fixedS):\n",
    "    data_fixedS_feats = data[:,fixedS[:,0]]*data[:,fixedS[:,1]]\n",
    "    return data_fixedS_feats # n_points x fixedS deg 2 terms\n",
    "\n",
    "\n",
    "def get_polynomial_regression_only_deg2_monomials_fixedS_reference_errors(xs, ys, batch_fixedS):\n",
    "    # xs -> batch x n_points x n_dim_input\n",
    "    # ys -> batch x n_points\n",
    "    # fixedS -> [[1,3], [4,5],..] |S| x 2 -- monomial term indices\n",
    "    # Least Squares Optimization\n",
    "    lsq_errors = []\n",
    "    lsq_preds = []\n",
    "    for i in tqdm(range(1, xs.shape[1])): # 1-19\n",
    "        preds = []\n",
    "        for batch_id in range(xs.shape[0]): # 0-63\n",
    "            # fit n_points -1 regressors for each entry in batch\n",
    "            xs_feats = xs[batch_id,:i] # (i, n_dim_input)\n",
    "            ys_feats = ys[batch_id,:i] # (i,)\n",
    "            deg2_monomial_feats = transform_features_to_only_fixedS_deg2_monomials(xs_feats, batch_fixedS[batch_id])\n",
    "            polyreg_fit = LinearRegression(fit_intercept = False).fit(deg2_monomial_feats, ys_feats)\n",
    "            polyreg_pred = polyreg_fit.predict(\n",
    "                transform_features_to_only_fixedS_deg2_monomials(xs[batch_id,i:i+1], batch_fixedS[batch_id])\n",
    "                                               )[0]\n",
    "            preds.append(\n",
    "                polyreg_pred\n",
    "            )\n",
    "        preds = np.array(preds).squeeze()\n",
    "        lsq_errors.append(((ys[:,i] - preds)**2).numpy())\n",
    "        lsq_preds.append(preds)\n",
    "        # lsq_errors.append(((ys[:,i] - preds)**2).mean(axis = 0).numpy())\n",
    "    return np.array(lsq_errors).T, np.array(lsq_preds).T\n",
    "\n",
    "# fit a degree 2 poly that looks like w1.x1.x1 + w2.x1.x2 +.....+ wk.xd.xd\n",
    "# where xi for i=1 to 20 are indices in input x (20 variables) i.e. d=20,\n",
    "# and weights wj for j=1 to (20C1 + 20C2); 20C1 terms x1.x1, x2.x2, ...,x20.x20; 20C2 terms x1.x2 + x1.x3 +... \n",
    "def get_polynomial_regression_only_deg2_monomials_reference_errors(xs, ys):\n",
    "    # xs -> batch x n_points x n_dim_input\n",
    "    # ys -> batch x n_points\n",
    "    # Least Squares Optimization\n",
    "    lsq_errors = []\n",
    "    lsq_preds = []\n",
    "    for i in tqdm(range(1, xs.shape[1])): # 1-19\n",
    "        preds = []\n",
    "        for batch_id in range(xs.shape[0]): # 0-63\n",
    "            # fit n_points -1 regressors for each entry in batch\n",
    "            xs_feats = xs[batch_id,:i] # (i, n_dim_input)\n",
    "            ys_feats = ys[batch_id,:i] # (i,)\n",
    "            deg2_monomial_feats = transform_features_to_only_deg2_monomials(xs_feats)\n",
    "            polyreg_fit = LinearRegression(fit_intercept = False).fit(deg2_monomial_feats, ys_feats)\n",
    "            polyreg_pred = polyreg_fit.predict(\n",
    "                transform_features_to_only_deg2_monomials(xs[batch_id,i:i+1])\n",
    "                                               )[0]\n",
    "            preds.append(\n",
    "                polyreg_pred\n",
    "            )\n",
    "        preds = np.array(preds).squeeze()\n",
    "        lsq_errors.append(((ys[:,i] - preds)**2).numpy())\n",
    "        lsq_preds.append(preds)\n",
    "        # lsq_errors.append(((ys[:,i] - preds)**2).mean(axis = 0).numpy())\n",
    "    return np.array(lsq_errors).T, np.array(lsq_preds).T\n",
    "\n",
    "# fit a degree 2 poly that looks like w0 + w1.x1.x1 + w2.x1.x2 +.....+ wk.xd.xd\n",
    "# where xi for i=1 to 20 are indices in input x (20 variables) i.e. d=20,\n",
    "# and weights wj for j=0 to (20C1 + 20C2); 20C1 terms x1.x1, x2.x2, ...,x20.x20; 20C2 terms x1.x2 + x1.x3 +... \n",
    "def get_polynomial_regression_upto_deg2_monomials_reference_errors(xs, ys):\n",
    "    # xs -> batch x n_points x n_dim_input\n",
    "    # ys -> batch x n_points\n",
    "    # Least Squares Optimization\n",
    "    lsq_errors = []\n",
    "    for i in tqdm(range(1, xs.shape[1])): # 1-19\n",
    "        preds = []\n",
    "        for batch_id in range(xs.shape[0]): # 0-63\n",
    "            # fit n_points -1 regressors for each entry in batch\n",
    "            xs_feats = xs[batch_id,:i] # (i, n_dim_input)\n",
    "            ys_feats = ys[batch_id,:i] # (i,)\n",
    "            deg2_monomial_feats = transform_features_to_upto_deg2_monomials(xs_feats)\n",
    "            polyreg_fit = LinearRegression(fit_intercept = False).fit(deg2_monomial_feats, ys_feats)\n",
    "            polyreg_pred = polyreg_fit.predict(\n",
    "                transform_features_to_upto_deg2_monomials(xs[batch_id,i:i+1])\n",
    "                                               )[0]\n",
    "            preds.append(\n",
    "                polyreg_pred\n",
    "            )\n",
    "        preds = np.array(preds).squeeze()\n",
    "        lsq_errors.append(((ys[:,i] - preds)**2).mean(axis = 0).numpy())\n",
    "    return np.array(lsq_errors)\n",
    "\n",
    "def get_sparse_regression_all_deg2_monomials_reference_errors(xs, ys, alpha=0.01, max_iter=100000):\n",
    "    lasso_errors = []\n",
    "    lasso_preds = []\n",
    "    for i in tqdm(range(1, xs.shape[1])):\n",
    "        preds = []\n",
    "        for batch_id in range(xs.shape[0]):\n",
    "            xs_feats = xs[batch_id,:i] # (i, n_dim_input)\n",
    "            ys_feats = ys[batch_id,:i] # (i,)\n",
    "            deg2_monomial_feats = transform_features_to_only_deg2_monomials(xs_feats)\n",
    "            # if i < 5: # why?\n",
    "            preds.append(\n",
    "            Lasso(alpha=alpha, fit_intercept = False, max_iter=max_iter).fit(deg2_monomial_feats, ys_feats)\\\n",
    "                .predict(transform_features_to_only_deg2_monomials(xs[batch_id,i:i+1]))[0]\n",
    "            )\n",
    "            # else:\n",
    "            #     preds.append(\n",
    "            #     LassoCV(fit_intercept = False).fit(deg2_monomial_feats, ys_feats)\\\n",
    "            #         .predict(transform_features_to_only_deg2_monomials(xs[batch_id,i:i+1]))[0]\n",
    "            #     )\n",
    "        preds = np.array(preds).squeeze()\n",
    "        lasso_errors.append(((ys[:,i] - preds)**2).numpy())\n",
    "        lasso_preds.append(preds)\n",
    "        # lasso_errors.append(((ys[:,i] - preds)**2).mean(axis = 0).numpy())\n",
    "    return np.array(lasso_errors).T, np.array(lasso_preds).T\n",
    "\n",
    "def get_sparse_regression_reference_errors(xs, ys):\n",
    "    lasso_errors = []\n",
    "    for i in tqdm(range(1, xs.shape[1])):\n",
    "        preds = []\n",
    "        for batch_id in range(xs.shape[0]):\n",
    "            if i < 5: # why?\n",
    "                preds.append(\n",
    "                Lasso(fit_intercept = False).fit(xs[batch_id,:i], ys[batch_id,:i])\\\n",
    "                    .predict(xs[batch_id,i:i+1])[0]\n",
    "                )\n",
    "            else:\n",
    "                preds.append(\n",
    "                LassoCV(fit_intercept = False).fit(xs[batch_id,:i], ys[batch_id,:i])\\\n",
    "                    .predict(xs[batch_id,i:i+1])[0]\n",
    "                )\n",
    "        preds = np.array(preds).squeeze()\n",
    "        lasso_errors.append(((ys[:,i] - preds)**2).mean(axis = 0).numpy())\n",
    "    return np.array(lasso_errors)\n",
    "\n",
    "def get_sign_vec_cs_reference_errors(xs, ys, n_dims):\n",
    "    # Inf Norm Optimization\n",
    "    mat_dim = int(np.sqrt(xs.shape[2]))\n",
    "    baseline_errors_batch = []\n",
    "    for b in tqdm(range(xs.shape[0])):\n",
    "        errors = []\n",
    "        for t in range(xs.shape[1] - 1):\n",
    "            w_star = Variable([n_dims, 1])\n",
    "            obj = Minimize(cvxnorm(w_star, 'inf'))\n",
    "            constraints = [ys[b,:t+1].numpy()[:,np.newaxis] == (xs[b,:t+1].numpy() @ w_star)]\n",
    "            prob = Problem(obj, constraints)\n",
    "            result = prob.solve()#verbose=True)\n",
    "            if prob.status == cvxpy.OPTIMAL:\n",
    "                pred = w_star.value[:,0] @ xs[b,t+1].numpy()\n",
    "                errors.append((pred - ys[b,t+1].numpy())**2)\n",
    "            else:\n",
    "                errors.append(prob.value)\n",
    "        baseline_errors_batch.append(errors)\n",
    "    return np.array(baseline_errors_batch).mean(0)\n",
    "\n",
    "def plot_results(\n",
    "    transformer_loss: list, \n",
    "    plot_title,\n",
    "    task_losses: dict,\n",
    "    task_kwargs: dict = {}\n",
    "):\n",
    "    x_axis_items = np.arange(1, transformer_loss.shape[0]+1)\n",
    "    plt.plot(x_axis_items, transformer_loss, lw=2, label=\"Transformer\")\n",
    "    if \"sign_vec_cs\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"sign_vec_cs\"], label = \"Inf Norm Minimization\")\n",
    "        plt.scatter(task_kwargs[\"sign_vec_cs\"][\"bound\"] + 1,0, color=\"red\", label=\"Bound\")\n",
    "    if \"linear_regression\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"linear_regression\"], lw=2, label = \"Least Squares\")\n",
    "    if \"sparse_regression\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"sparse_regression\"], lw=2, label = \"Lasso\")\n",
    "    if \"monomial_deg2\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"monomial_deg2\"], lw=2, label = \"L.Sq. - all deg 2 monomials\")\n",
    "    if \"monomial_deg0_1_2\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"monomial_deg0_1_2\"], lw=2, label = \"L.Sq. - all deg 0,1,2 monomials\")\n",
    "    if \"monomial_deg2_fixedS\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"monomial_deg2_fixedS\"], lw=2, label = \"L.Sq. - all deg 2 fixed S monomials\")\n",
    "    if \"2-layerNN-GD\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"2-layerNN-GD\"], lw=2, label = \"2-layer NN, GD\")\n",
    "    if \"lasso_all_deg2_monomials\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"lasso_all_deg2_monomials\"], lw=2, label = \"Lasso - all deg 2 monomials\")\n",
    "    if \"haar_basis\" in task_losses:\n",
    "        plt.plot(x_axis_items, task_losses[\"haar_basis\"], lw=2, label = \"OLS on Haar Basis features (max_level=3)\")\n",
    "\n",
    "    plt.xlabel(\"# in-context examples\")\n",
    "    plt.ylabel(\"squared error\")\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_results_single_curve(\n",
    "    loss: list, \n",
    "    loss_label: str\n",
    "):\n",
    "    x_axis_items = np.arange(1, loss.shape[0]+1)\n",
    "    plt.plot(x_axis_items, loss, lw=2, label=loss_label)\n",
    " \n",
    "    plt.xlabel(\"# in-context examples\")\n",
    "    plt.ylabel(\"squared error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_average_loss_difference(loss1, loss2, losses_names):\n",
    "    assert loss1.shape == loss2.shape, \"loss1 and loss2 must have same length\"\n",
    "    print(f\"Average difference between {losses_names} losses:\", np.abs(loss1 - loss2).mean())\n",
    "\n",
    "def average_loss_from_index(ind, loss, loss_name):\n",
    "    print(f\"Average {loss_name} loss from index {ind} is:\", loss[ind:].mean())\n",
    "\n",
    "def average_loss_after_seeing_at_least_k_examples(k, loss, loss_name):\n",
    "    print(f\"Average {loss_name} loss after seeing {k} examples in prompt is:\", loss[k-1:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KS_monomial_sets import monomial_terms\n",
    "\n",
    "\n",
    "def get_model_from_run_folder(run_folder_path):\n",
    "    # the only folder in run_folder_path is the run_id folder\n",
    "    run_id = os.listdir(run_folder_path)[0]\n",
    "    run_path = os.path.join(run_folder_path, run_id)\n",
    "    recompute_metrics = False\n",
    "\n",
    "    print(f\"Loading model from {run_folder_path}/{run_id}...\")\n",
    "    model, conf = get_model_from_run(run_path)\n",
    "\n",
    "    numDeg2Select = conf.training.task_kwargs[\"numDeg2Select\"]\n",
    "    sizeOfK = conf.training.task_kwargs[\"sizeOfK\"]\n",
    "    conf.training.task_kwargs[\"fixedK\"] = torch.tensor(monomial_terms[f\"{conf.model.n_dims}-{sizeOfK}-{numDeg2Select}\"], dtype=torch.int64)\n",
    "\n",
    "    return model, conf \n",
    "\n",
    "def get_model_from_run_folder_usual(run_folder_path):\n",
    "    # the only folder in run_folder_path is the run_id folder\n",
    "    run_id = os.listdir(run_folder_path)[0]\n",
    "    run_path = os.path.join(run_folder_path, run_id)\n",
    "\n",
    "    print(f\"Loading model from {run_folder_path}/{run_id}...\")\n",
    "    model, conf = get_model_from_run(run_path)\n",
    "    \n",
    "    return model, conf \n",
    "\n",
    "\n",
    "def do_task_config(conf, n_dims, eval_batch_size):\n",
    "    data_kwargs = conf.training.data_kwargs\n",
    "    if data_kwargs is None:\n",
    "        data_kwargs = {}\n",
    "\n",
    "    # task_kwargs = copy.deepcopy(conf.training.task_kwargs)\n",
    "    all_deg2_terms = get_all_deg2_term_indices(n_dims)\n",
    "    conf.training.task_kwargs[\"all_deg2_terms\"] = all_deg2_terms\n",
    "    data_sampler = get_data_sampler(conf.training.data, n_dims, **data_kwargs)\n",
    "    task_sampler = get_task_sampler(\n",
    "        conf.training.task,\n",
    "        n_dims,\n",
    "        eval_batch_size,\n",
    "        num_tasks=conf.training.num_tasks,\n",
    "        **conf.training.task_kwargs\n",
    "    )\n",
    "\n",
    "    return data_sampler, task_sampler\n",
    "\n",
    "def get_monomial_reference_errors(sizeOfK, xs, ys, conf, K_used=None, is_ref=[\"only2\",\"upto2\",\"onlyfixedS\",\"lasso\"], lasso_alpha=0.01):\n",
    "    # reference is a degree 2 poly with (20 self sq. + 190 cross) terms that looks like w1.x1.x1 + w2.x2.x2 +.....+ w21.x1.x2 + w22.x2.x3.....+ w210.x19.x20\n",
    "    # where xi for i=1 to 20 are indices in input x\n",
    "    # and weights wj for j=1 to (20C1 + 20C2)=210; 20C1 terms x1.x1, x2.x2, ...,x20.x20; 20C2 terms x1.x2 + x1.x3 +... \n",
    "    lsq_errors_only_deg_2=None\n",
    "    lsq_errors_upto_deg_2=None\n",
    "    lsq_errors_only_deg_2_fixedS=None\n",
    "    lasso_errors=None\n",
    "    if \"only2\" in is_ref:\n",
    "        lsq_errors_only_deg_2, _ = get_polynomial_regression_only_deg2_monomials_reference_errors(xs, ys)\n",
    "        # B x P\n",
    "    # reference is a degree 2 poly with (1 bias + 20 deg 1 + 20 self sq. + 190 cross) terms that looks like w0 + w1.x1 + w2.x2 + ... + w20.x20 + w21.x1.x1 + w22.x2.x2 +.....+ w41.x1.x2 + w42.x2.x3.....+ w230.x19.x20\n",
    "    # where xi for i=1 to 20 are indices in input x\n",
    "    # and weights wj for j=0 to (20C1 + 20C1 + 20C2)=230; 1 bias term; 20C1 (deg 1 terms) x1, x2, ... x20; 20C1 (deg 2 self sq.) terms x1.x1, x2.x2, ...,x20.x20; 20C2 (cross deg 2) terms x1.x2 + x1.x3 +... \n",
    "    if \"upto2\" in is_ref:\n",
    "        lsq_errors_upto_deg_2  = get_polynomial_regression_upto_deg2_monomials_reference_errors(xs, ys)\n",
    "    if \"onlyfixedS\" in is_ref:\n",
    "        lsq_errors_only_deg_2_fixedS, _ = get_polynomial_regression_only_deg2_monomials_fixedS_reference_errors(xs, ys, np.array(K_used))\n",
    "        # B x P\n",
    "    if \"lasso\" in is_ref:\n",
    "        lasso_errors, _ = get_sparse_regression_all_deg2_monomials_reference_errors(xs, ys, alpha=lasso_alpha)\n",
    "        # B x P\n",
    "\n",
    "    return {\n",
    "        \"only2\":lsq_errors_only_deg_2,\n",
    "        \"upto2\":lsq_errors_upto_deg_2,\n",
    "        \"onlyfixedS\":lsq_errors_only_deg_2_fixedS,\n",
    "        \"lasso\":lasso_errors\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_and_plot_ID(model, bp_proxy_tf, conf, data_sampler, task_sampler, len_deg2_monomials, sizeOfK, eval_reference_list, eval_bs, eval_pts, lasso_alpha):\n",
    "    print(\"task:\", conf.training.task)\n",
    "    task = task_sampler()\n",
    "    xs = data_sampler.sample_xs(b_size=eval_bs, n_points=eval_pts)\n",
    "    \n",
    "    chosenSindices = torch.randint(0, sizeOfK, (eval_bs,))\n",
    "    ys = task.evaluate(xs, mode='eval', chosenSindices=chosenSindices) # ID eval\n",
    "    K_used = conf.training.task_kwargs[\"fixedK\"][chosenSindices]\n",
    "    print(\"current task:\", task)\n",
    "    print(\"xs shape:\", xs.shape)\n",
    "    print(\"ys shape:\", ys.shape)\n",
    "\n",
    "    assert len(task.all_deg2_terms) == len_deg2_monomials\n",
    "    model.eval()\n",
    "    bp_proxy_tf.eval()\n",
    "    with torch.no_grad():\n",
    "        transformer_pred = model(xs, ys)\n",
    "        bp_proxy_pred = bp_proxy_tf(xs, ys)\n",
    "    \n",
    "    ref_errors_dict=get_monomial_reference_errors(sizeOfK, xs, ys, conf, K_used, is_ref=eval_reference_list, lasso_alpha=lasso_alpha)\n",
    "    return ref_errors_dict, task, transformer_pred, bp_proxy_pred, ys\n",
    "\n",
    "def eval_and_plot_OOD(model, bp_proxy_tf, conf, data_sampler, task_sampler, len_deg2_monomials, sizeOfK, eval_reference_list, eval_bs, eval_pts, lasso_alpha):\n",
    "    print(\"task:\", conf.training.task)\n",
    "    task = task_sampler()\n",
    "    xs = data_sampler.sample_xs(b_size=eval_bs, n_points=eval_pts)\n",
    "    \n",
    "    ys = task.evaluate_ood(xs) # OOD eval\n",
    "    K_used = task.selected_monomial_indices\n",
    "    print(\"current task:\", task)\n",
    "    print(\"xs shape:\", xs.shape)\n",
    "    print(\"ys shape:\", ys.shape)\n",
    "\n",
    "    assert len(task.all_deg2_terms) == len_deg2_monomials\n",
    "    model.eval()\n",
    "    bp_proxy_tf.eval()\n",
    "    with torch.no_grad():\n",
    "        transformer_pred = model(xs, ys)\n",
    "        bp_proxy_pred = bp_proxy_tf(xs, ys)\n",
    "    \n",
    "    ref_errors_dict=get_monomial_reference_errors(sizeOfK, xs, ys, conf, K_used, is_ref=eval_reference_list, lasso_alpha=lasso_alpha)\n",
    "    return ref_errors_dict, task, transformer_pred, bp_proxy_pred, ys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "sns.set_theme('notebook', 'darkgrid')\n",
    "palette = sns.color_palette('colorblind')\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "# mpl.rcParams['text.usetex'] = True\n",
    "\n",
    "matplotlib.rcParams.update({\n",
    "    'axes.titlesize': 8,\n",
    "  'figure.titlesize': 10, # was 10\n",
    "  'legend.fontsize': 10, # was 10\n",
    "  'xtick.labelsize': 6,\n",
    "  'ytick.labelsize': 6,\n",
    "})\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "def get_df_from_pred_array(pred_arr, n_points, offset = 0):\n",
    "    # pred_arr --> b x pts-1\n",
    "    batch_size=pred_arr.shape[0]\n",
    "    flattened_arr = pred_arr.ravel()\n",
    "    points = np.array(list(range(offset, n_points)) * batch_size)\n",
    "    df = pd.DataFrame({'y': flattened_arr, 'x': points})\n",
    "    return df\n",
    "\n",
    "def lineplot_with_ci(pred_or_err_arr, n_points, offset, label, ax, seed):\n",
    "    sns.lineplot(data=get_df_from_pred_array(pred_or_err_arr, n_points=n_points, offset = offset), \n",
    "                y=\"y\", x=\"x\",\n",
    "                label=label, \n",
    "                ax=ax, n_boot=1000, \n",
    "                seed=seed, \n",
    "                ci=90\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latex(plot_dict, K, is_id, alpha, y_limit=None):\n",
    "    plt_title = f\"$K={K}$, \"\n",
    "    seed=42\n",
    "    if is_id:\n",
    "        ID_OOD = \"ID\"\n",
    "        plt_title += \"ID Evaluation\"\n",
    "    else:\n",
    "        ID_OOD = \"OOD\"\n",
    "        plt_title += \"OOD Evaluation\"\n",
    "    \n",
    "    sns.set(style = \"whitegrid\", font_scale=1.5)\n",
    "\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(7, 5), constrained_layout=True)\n",
    "\n",
    "    for k, v in plot_dict.items():\n",
    "        if v is not None:\n",
    "            print(k, v.shape)\n",
    "            lineplot_with_ci(v, n_points, offset = 1, label=k, ax=ax1, seed=seed)\n",
    "    \n",
    "    ax1.set_xlabel(\"$k$ (# in-context examples)\")\n",
    "    ax1.set_ylabel(\"$\\\\mathrm{loss@k}$\")\n",
    "    \n",
    "    ax1.legend(fontsize=15)\n",
    "    if y_limit is not None:\n",
    "        ax1.set_ylim(y_limit[0], y_limit[1])\n",
    "    plt.title(plt_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models such that they save at path some/path/prefix/polynomials/descriptive_model_folder_name/run_id_uuid\n",
    "models_log_dir = \"some/path/prefix\"\n",
    "n_dims=10\n",
    "eval_batch_size = 1280\n",
    "n_points = 125\n",
    "len_deg2_monomials = 55\n",
    "\n",
    "lasso_alpha = 0.1\n",
    "y_limit = [-0.125, 1.625]\n",
    "\n",
    "K=[10, 20, 40, 100, 500, 1000, 5000]\n",
    "n_points_tr = 125\n",
    "sizeOfS = 10\n",
    "\n",
    "bp_proxy_tf, _ = get_model_from_run_folder_usual(os.path.join(models_log_dir, \"polynomials\", \"descriptive_model_folder_name-for-bp-proxy-model\"))\n",
    "\n",
    "\n",
    "for sizeOfK in K:\n",
    "    run_folder_name = f\"descriptive_model_folder_name_pos130_d10_p{n_points_tr}_S{sizeOfS}_K{sizeOfK}\"\n",
    "    run_folder_path = os.path.join(models_log_dir, \"polynomials\", run_folder_name)\n",
    "    model, conf = get_model_from_run_folder(run_folder_path)\n",
    "\n",
    "    # config for the run\n",
    "    print(dict(conf.training))\n",
    "    data_sampler, task_sampler = do_task_config(conf, n_dims, eval_batch_size)\n",
    "\n",
    "    # eval ID on batch of S\n",
    "    eval_reference_list=[\"only2\", \"onlyfixedS\", \"lasso\"]\n",
    "\n",
    "    ref_errors_dict, task, transformer_pred, bp_proxy_pred, ys = eval_and_plot_ID(model, bp_proxy_tf, conf, data_sampler, task_sampler, len_deg2_monomials, sizeOfK, eval_reference_list, eval_bs=eval_batch_size, eval_pts=n_points, lasso_alpha=lasso_alpha)\n",
    "    \n",
    "    plot_title_str=f\"|K|={sizeOfK}, |S|={sizeOfS}, p={n_points_tr}\\nID: All S's are from K\"\n",
    "\n",
    "    metric = task.get_metric()\n",
    "    transformer_loss = metric(transformer_pred, ys).numpy()[:,1:]\n",
    "    bp_proxy_loss = metric(bp_proxy_pred, ys).numpy()[:,1:]\n",
    "\n",
    "    # each tensor value must be batch x npoints\n",
    "    plot_dict = {\n",
    "        \"HMICL TF\": transformer_loss,\n",
    "        \"$\\\\mathrm{OLS}_{\\mathcal{S}}$\": ref_errors_dict[\"onlyfixedS\"],\n",
    "        \"$\\\\mathrm{OLS}_{\\Phi_{M}}$\": ref_errors_dict[\"only2\"],\n",
    "        \"$\\\\mathrm{Lasso}_{\\Phi_{M}}$\": ref_errors_dict[\"lasso\"],\n",
    "        \"$\\\\mathrm{BP}_{\\mathrm{proxy}}$\": bp_proxy_loss\n",
    "    }\n",
    "\n",
    "    plot_latex(plot_dict, sizeOfK, is_id=True, alpha=lasso_alpha, y_limit=y_limit)\n",
    "\n",
    "    # eval OOD\n",
    "    eval_reference_list=[\"only2\", \"onlyfixedS\", \"lasso\"]\n",
    "\n",
    "    ref_errors_dict, task, transformer_pred, bp_proxy_pred, ys=eval_and_plot_OOD(model, bp_proxy_tf, conf, data_sampler, task_sampler, len_deg2_monomials, sizeOfK, eval_reference_list, eval_bs=eval_batch_size, eval_pts=n_points, lasso_alpha=lasso_alpha)\n",
    "    \n",
    "    plot_title_str=f\"|K|={sizeOfK}, |S|={sizeOfS}, p={n_points_tr}\\nOOD: All S's are random\"\n",
    "\n",
    "    metric = task.get_metric()\n",
    "    transformer_loss = metric(transformer_pred, ys).numpy()[:,1:]\n",
    "    bp_proxy_loss = metric(bp_proxy_pred, ys).numpy()[:,1:]\n",
    "\n",
    "    # each tensor value must be batch x npoints\n",
    "    plot_dict = {\n",
    "        \"HMICL TF\": transformer_loss,\n",
    "        \"$\\\\mathrm{OLS}_{\\mathcal{S}}$\": ref_errors_dict[\"onlyfixedS\"],\n",
    "        \"$\\\\mathrm{OLS}_{\\Phi_{M}}$\": ref_errors_dict[\"only2\"],\n",
    "        \"$\\\\mathrm{Lasso}_{\\Phi_{M}}$\": ref_errors_dict[\"lasso\"],\n",
    "        \"$\\\\mathrm{BP}_{\\mathrm{proxy}}$\": bp_proxy_loss\n",
    "    }\n",
    "\n",
    "    plot_latex(plot_dict, sizeOfK, is_id=False, alpha=lasso_alpha, y_limit=y_limit)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
